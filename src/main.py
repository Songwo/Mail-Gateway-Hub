import asyncio
import argparse
import json
import sqlite3
import sys
import traceback
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, Optional

import requests
from google import genai
from imap_tools import MailBox, AND
from loguru import logger

# ===========================
# 0. å…¨å±€å˜é‡ä¸æ—¥å¿—é…ç½®
# ===========================
SCRIPT_START_TIME = datetime.now()
DB_PATH = "mail_gateway.db"
CONFIG_FILE = "config.json"
MODEL_INIT_DONE = False

logger.remove()
logger.add(sys.stdout, colorize=True, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>")
logger.add("mail_gateway.log", rotation="10 MB", retention="7 days")

# ===========================
# 1. æ ¸å¿ƒé…ç½®ä¸æ•°æ®åº“
# ===========================
def load_config() -> Dict:
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ [CONFIG] é…ç½®æ–‡ä»¶ '{CONFIG_FILE}' æœªæ‰¾åˆ°ï¼")
        sys.exit(1)

def init_db():
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute('''
            CREATE TABLE IF NOT EXISTS processed_emails (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                account_email TEXT NOT NULL,
                alias TEXT,
                uid TEXT NOT NULL,
                category TEXT,
                summary TEXT,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(account_email, uid)
            )
        ''')
        try:
            conn.execute("ALTER TABLE processed_emails ADD COLUMN alias TEXT")
        except sqlite3.OperationalError:
            pass 

def is_processed(email: str, uid: str) -> bool:
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        cur.execute("SELECT 1 FROM processed_emails WHERE account_email=? AND uid=?", (email, uid))
        return cur.fetchone() is not None

def save_result(email: str, alias: str, uid: str, ai_result: Dict):
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            INSERT OR IGNORE INTO processed_emails 
            (account_email, alias, uid, category, summary) 
            VALUES (?, ?, ?, ?, ?)
        """, (email, alias, uid, ai_result.get('category'), ai_result.get('summary')))

def get_db_stats(limit=15):
    with sqlite3.connect(DB_PATH) as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute("SELECT * FROM processed_emails ORDER BY processed_at DESC LIMIT ?", (limit,))
        return [dict(row) for row in cursor.fetchall()]

# ===========================
# 2. å¼‚æ­¥ IO åŒ…è£… (AI & æ¨é€)
# ===========================
executor = ThreadPoolExecutor(max_workers=10)

async def async_call_gemini(content: str, config: Dict) -> Dict:
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, _sync_call_gemini, content, config)

def _sync_call_gemini(content: str, config: Dict) -> Dict:
    api_key = config.get("gemini_api_key")
    if not api_key:
        return {"category": "æœªé…ç½®AI", "summary": "æœªé…ç½® Gemini API Key", "priority": 1}
    
    # ä»£ç†é…ç½®
    proxy = config.get("use_proxy")
    if proxy:
        import os
        os.environ["HTTP_PROXY"] = proxy
        os.environ["HTTPS_PROXY"] = proxy
        logger.info(f"ğŸŒ [AI] ä½¿ç”¨ä»£ç†: {proxy}")

    config_model = config.get("gemini_model")
    if config_model:
        target_models = [config_model]
    else:
        target_models = [
            'gemini-2.5-flash',
            'gemini-2.5-flash-latest',
            'gemini-2.5-pro',
            'gemini-1.5-flash-latest',
            'gemini-1.5-flash',
        ]
    model_to_use = "gemini-1.5-flash-latest"
    
    client = genai.Client(api_key=api_key)
    try:
        global MODEL_INIT_DONE
        if not MODEL_INIT_DONE and not config_model:
            detected = _detect_first_available_model(client, target_models)
            if detected:
                config["gemini_model"] = detected
                _write_config(config)
                target_models = [detected]
                logger.info(f"âœ… [AI] è‡ªåŠ¨æ£€æµ‹å¯ç”¨æ¨¡å‹: {detected}ï¼Œå·²å†™å…¥é…ç½®")
            MODEL_INIT_DONE = True

        prompt = f"{config.get('system_prompt', '')}\nEmail Content: {content[:3000]}\nOutput JSON ONLY."
        last_error = None
        for tm in target_models:
            try:
                model_to_use = tm
                resp = client.models.generate_content(model=tm, contents=prompt)
                text = (resp.text or "").replace('```json', '').replace('```', '').strip()
                return json.loads(text)
            except Exception as e:
                logger.warning(f"âš ï¸ [AI] æ¨¡å‹ä¸å¯ç”¨æˆ–è°ƒç”¨å¤±è´¥: {tm} | {type(e).__name__}")
                last_error = e
                continue
        raise last_error if last_error else RuntimeError("æœªèƒ½è°ƒç”¨ä»»ä½•å¯ç”¨æ¨¡å‹")
    except Exception:
        logger.error(f"âŒ [AI] Gemini è°ƒç”¨å¤±è´¥ (å°è¯•æ¨¡å‹: {model_to_use})")
        logger.error(traceback.format_exc())
        return {"category": "AI Error", "summary": "è§£æå¤±è´¥", "priority": 1}
    finally:
        try:
            client.close()
        except Exception:
            pass

def _detect_first_available_model(client: "genai.Client", candidates: List[str]) -> Optional[str]:
    try:
        available = set()
        for m in client.models.list():
            name = getattr(m, "name", "") or ""
            if name.startswith("models/"):
                available.add(name.replace("models/", ""))
        for c in candidates:
            if c in available:
                return c
    except Exception as e:
        logger.warning(f"âš ï¸ [AI] è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å¤±è´¥: {type(e).__name__}")
    return None

def _write_config(config: Dict) -> None:
    try:
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"âš ï¸ [CONFIG] å†™å…¥é…ç½®å¤±è´¥: {type(e).__name__}")

async def async_send_feishu(msg_data: Dict, ai_result: Dict, config: Dict, account_cfg: Dict):
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(executor, _sync_send_feishu, msg_data, ai_result, config, account_cfg)

def _sync_send_feishu(msg_data: Dict, ai_result: Dict, config: Dict, account_cfg: Dict):
    webhook = config.get("feishu_webhook")
    if not webhook: return

    alias = account_cfg.get('alias', 'é»˜è®¤')
    category = ai_result.get('category', 'å…¶ä»–')
    is_urgent = category in ["éªŒè¯ç ", "é‡è¦é€šçŸ¥"] or ai_result.get('priority', 0) >= 4
    header_color = "red" if is_urgent else "blue"
    
    email = account_cfg['email']
    summary = ai_result.get('summary', 'æ— æ‘˜è¦')
    v_code = ai_result.get('verification_code')
    
    content_md = f"**æ‘˜è¦**: {summary}\n**å‘ä»¶äºº**: {msg_data['from']}"
    if v_code and str(v_code).lower() != "null":
        content_md += f"\n\n**éªŒè¯ç **: <font color='red'>{v_code}</font>"

    payload = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "template": header_color,
                "title": {"tag": "plain_text", "content": f"[{alias}] {msg_data['subject']}"}
            },
            "elements": [
                {"tag": "div", "text": {"tag": "lark_md", "content": content_md}},
                {"tag": "hr"},
                {
                    "tag": "note",
                    "elements": [
                        {"tag": "plain_text", "content": f"ğŸ“ èº«ä»½: {alias} | è´¦å·: {email}\nğŸ¤– ç”± Mail-Gateway-Hub é©±åŠ¨"}
                    ]
                }
            ]
        }
    }
    try:
        requests.post(webhook, json=payload, timeout=10)
    except Exception:
        logger.error("âŒ [NOTIFY] é£ä¹¦æ¨é€ç½‘ç»œå¼‚å¸¸ï¼")

# ===========================
# 3. æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
# ===========================
async def check_account(acc: Dict, global_config: Dict):
    email = acc['email']
    alias = acc.get('alias', 'Unknown')
    server = acc['imap_server']
    
    logger.info(f"ğŸ” [CONN] æ­£åœ¨æ£€æŸ¥: [{alias}] ({email})...")
    
    loop = asyncio.get_running_loop()
    try:
        def fetch_unread():
            with MailBox(server).login(email, acc['password'], initial_folder=acc.get('folder', 'INBOX')) as mb:
                msgs = []
                # å¼ºåˆ¶æ‹‰å–æ‰€æœ‰ UNSEEN é‚®ä»¶ï¼Œé¿å…æ¼æ”¶
                seen_uids = set()
                for m in mb.fetch(AND(seen=False)):
                    seen_uids.add(m.uid)
                    msgs.append({
                        "uid": m.uid, "subject": m.subject, "from": m.from_,
                        "content": m.text or m.html or ""
                    })

                # æ”¾å®½æ—¶é—´æ ¡éªŒèŒƒå›´ï¼Œé¢å¤–æ‹‰å–æœ€è¿‘ 7 å¤©çš„æœªè¯»é‚®ä»¶ä½œå…œåº•
                since_date = (date.today() - timedelta(days=7))
                for m in mb.fetch(AND(seen=False, date_gte=since_date)):
                    if m.uid in seen_uids:
                        continue
                    msgs.append({
                        "uid": m.uid, "subject": m.subject, "from": m.from_,
                        "content": m.text or m.html or ""
                    })
                return msgs

        messages = await loop.run_in_executor(executor, fetch_unread)
        
        if not messages:
            logger.debug(f"âœ¨ [{alias}] æ— æ–°é‚®ä»¶ã€‚")
            return

        # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„ UID
        new_messages = [m for m in messages if not is_processed(email, m['uid'])]
        
        if not new_messages:
            logger.debug(f"âœ¨ [{alias}] é‚®ä»¶å·²åœ¨æ•°æ®åº“ä¸­ï¼Œè·³è¿‡ã€‚")
            return

        logger.success(f"ğŸ“© [{alias}] å‘ç° {len(new_messages)} å°æœªå¤„ç†æ–°é‚®ä»¶ï¼")

        for msg in new_messages:
            ai_result = await async_call_gemini(msg['content'], global_config)
            await async_send_feishu(msg, ai_result, global_config, acc)
            save_result(email, alias, msg['uid'], ai_result)
            logger.success(f"âœ… [{alias}] å¤„ç†æˆåŠŸ: {msg['subject']}")
            await asyncio.sleep(1) # é¢‘ç‡é™åˆ¶

    except Exception:
        logger.error(f"âŒ [{alias}] è¿æ¥æˆ–å¤„ç†é”™è¯¯ï¼")
        logger.error(traceback.format_exc())

async def scheduler(config: Dict, run_once: bool = False):
    accounts = config.get('accounts', [])
    if not accounts:
        logger.error("âŒ [SYSTEM] æœªé…ç½®è´¦å·ã€‚")
        return

    if run_once:
        logger.info("ğŸ•’ [SYSTEM] æ‰§è¡Œå•æ¬¡æ‰«ææ¨¡å¼...")
        await asyncio.gather(*[check_account(acc, config) for acc in accounts])
    else:
        logger.success(f"ğŸš€ [SYSTEM] æ­£åœ¨ç›‘å¬ {len(accounts)} ä¸ªé‚®ç®±...")
        while True:
            await asyncio.gather(*[check_account(acc, config) for acc in accounts])
            logger.info("ğŸ’“ [HEARTBEAT] ç›‘å¬ä¸­ï¼Œ30ç§’åä¸‹ä¸€è½®...")
            await asyncio.sleep(30)

# ===========================
# 5. å…¥å£
# ===========================
if __name__ == "__main__":
    init_db()
    parser = argparse.ArgumentParser(description="Mail-Gateway-Hub")
    parser.add_argument("--list", action="store_true", help="æ˜¾ç¤ºæœ€è¿‘å¤„ç†è®°å½•")
    parser.add_argument("--once", action="store_true", help="å•æ¬¡æ‰«æåé€€å‡º")
    args = parser.parse_args()

    if args.list:
        stats = get_db_stats()
        print(f"\n{'æ—¶é—´':<20} | {'åˆ«å':<10} | {'åˆ†ç±»':<10} | {'æ‘˜è¦'}")
        print("-" * 100)
        for s in stats:
            print(f"{s['processed_at']:<20} | {s['alias']:<10} | {s['category']:<10} | {s['summary']}")
        print()
        sys.exit(0)

    config = load_config()

    if args.once:
        asyncio.run(scheduler(config, run_once=True))
    else:
        try:
            asyncio.run(scheduler(config))
        except KeyboardInterrupt:
            logger.warning("ğŸ›‘ [SYSTEM] æœåŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
